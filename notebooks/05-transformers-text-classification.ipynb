{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef3e96c2-e827-41a4-b748-5dfda512c337",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juanhuguet/intro_to_nlp/blob/main/notebooks/05-transformers-text-classification.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabfd313-ca09-49f1-84d7-f6e571b06e27",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text classification using transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a19b44-17f1-4958-99bd-1f6897c939e8",
   "metadata": {},
   "source": [
    "We have seen the power and effectiveness of dense word embeddings.\n",
    "\n",
    "However, they have a weakness:\n",
    "\n",
    "* the **word representations are contextless** and\n",
    "* do **not capture long-term dependencies between words** due to the limited span of the sliding window.\n",
    "\n",
    "To illustrate this example, let’s imagine the next sentences:\n",
    "\n",
    "> An apple a day keeps the doctor away!\n",
    "> \n",
    "\n",
    "> My doctor buys Apple stocks every day\n",
    "> \n",
    "\n",
    "We, humans, can quickly differentiate between both “apples” as we are context aware.\n",
    "\n",
    "The first sentence refers to a fruit whereas the second refers to a popular company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a18c87-35c5-4163-a58e-019cb02a6b00",
   "metadata": {},
   "source": [
    "# Technology landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f719d618-7c7c-4c59-92b7-90a92b4f4559",
   "metadata": {},
   "source": [
    "[Transformers library](https://huggingface.co/docs/transformers/index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4605634f-6ab7-43ee-9379-a6e781edf18b",
   "metadata": {},
   "source": [
    "<img src=\"https://file.notion.so/f/f/003df94c-172d-46b4-9c84-4a2f90ef0ed1/b6f0d5fb-306a-41fd-9df7-f623e0fed832/Screenshot_2023-05-04_at_03.23.55.png?id=f370c67d-f17e-4a86-ad54-b12dbe892c6b&table=block&spaceId=003df94c-172d-46b4-9c84-4a2f90ef0ed1&expirationTimestamp=1705687200000&signature=ZKtSsNyKJMIBn5W_QzSFu5p5NQhp2ncr6Epj_qkX48w&downloadName=Screenshot+2023-05-04+at+03.23.55.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e83f5f1-712e-4d9a-8ddb-410b7a071c5c",
   "metadata": {},
   "source": [
    "Whenever you face a new NLP task/challenge, review what has been already done by the community, choose the best base model and adapt it to your needs. Most of the times you will find an out-of-the-box solution that performs well enough or that needs little fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e180450e-915e-4605-aa5d-62e4ab8c0e20",
   "metadata": {},
   "source": [
    "## Transfer learning and fine tuning\n",
    "\n",
    "**Pre-training**, **fine-tuning**, and **transfer learning**:\n",
    "\n",
    "Using large transformer models has become the basis for many state-of-the-art NLP models.\n",
    "\n",
    "By pre-training a large transformer model on massive amounts of text data, and then fine-tuning it on smaller datasets for specific tasks, it is possible to achieve highly effective transfer learning. \n",
    "\n",
    "This approach has been used to achieve state-of-the-art results on a wide range of NLP tasks, including language modeling, sentiment analysis, machine translation, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ea5f9-3f63-4f0c-a7d0-442d3c63dae3",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "\n",
    "The key difference between fine-tuning and transfer learning is that fine-tuning involves adjusting the pre-trained model's parameters to fit a specific task, while transfer learning involves applying the pre-trained model to a new task without further training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f897a4-fc2a-4da1-b6f4-e380a52d4a19",
   "metadata": {},
   "source": [
    "<img src=\"https://file.notion.so/f/f/003df94c-172d-46b4-9c84-4a2f90ef0ed1/00ccfd76-c52d-45e8-9241-83523de6f02d/Screenshot_2023-05-04_at_03.20.21.png?id=ee4650de-c979-4395-b519-129e4e0d937f&table=block&spaceId=003df94c-172d-46b4-9c84-4a2f90ef0ed1&expirationTimestamp=1705687200000&signature=jrq6g5xV0wH8eLlpNCZAbLXMwDkGYMQlFkXYYKKMNjQ&downloadName=Screenshot+2023-05-04+at+03.20.21.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df553c70-8941-4129-99f2-26823d4a66a2",
   "metadata": {},
   "source": [
    "# Let's preload some dataset...\n",
    "\n",
    "Check the datasets available from hugging face hub:\n",
    "\n",
    "https://huggingface.co/datasets\n",
    "\n",
    "We will choose the `yelp` reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576eac78-edcc-469f-8e82-63f4e6f3f967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba073cc-dc51-43d8-82c3-7fccd42621a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf5cf18-e55d-459f-b5fd-292c671699eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab34e95-e196-4e64-bb7e-8163a2bcfc02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8520f767-9df7-46da-b5b0-494f186bf495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a94a83-80b2-4b17-8830-f12d5a1d66bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Out of the box classifier\n",
    "\n",
    "* Transformers has a layered API that allows you to interact with the library at various levels of abstraction.\n",
    "\n",
    "* `pipelines` abstract away all the steps needed to convert raw text into a set of predictions from a fine-tuned model\n",
    "\n",
    "* `pipelines` support many of out-of-the box nlp tasks and can be used with a variety of models\n",
    "\n",
    "[model for sentiment classification](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493af1e9-3ec8-452d-a0da-9cd0f283aded",
   "metadata": {},
   "source": [
    "[behind the pipeline](https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9c4fa-5951-47da-93cd-77450c730d69",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg\" width=\"800\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44259b7b-4b21-4bf3-83dd-cedb00dd6d46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "#use a text classification pipeline\n",
    "classifier = pipeline(...,\n",
    "                      model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0796846-8b57-4c23-b84d-5fce3a1e8e00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc7588-55f9-4687-8663-61bee550793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefd07a-9762-4d00-9481-d8947865cc18",
   "metadata": {},
   "source": [
    "* We see that there is a good approach here, however, it lacks the granularity we may have in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cbd6f7-89ae-4890-a7c0-52ab66921558",
   "metadata": {},
   "source": [
    "## Fine tuning your own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cabfe1-38da-4c1a-bdd8-62c2be96f043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = \"distilbert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a342bc1-2ef7-4fee-9657-43652503c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e19325-d68c-4302-9b51-6b1b6bbd836f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7577e2eb-242f-4669-85ff-ef20600e15cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c9ee3d-8baf-4914-ad78-6b5f37f4cd30",
   "metadata": {},
   "source": [
    "### Let's inspect what has the tokenizer done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160af32-dd26-4173-959e-bcd6a0ee66a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f1d102-eb07-43f5-b010-3505e655566c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b0527-7765-454b-bbb2-6ddbbcf8355d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42800940-3054-4bc5-bebe-b1b7c9ef631e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2cdee-e88d-47bf-9434-cd0a926a150b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(1917)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cfc010-87cd-42b7-a720-721dc073ff6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28910527-3a61-4b94-b9df-eab1015f22d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.vocab[\"everything\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db5b28-1de1-44db-80dc-4d348dd95b9e",
   "metadata": {},
   "source": [
    "## Now we have our text tokenized, let's train our model using the high level wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8bf144-0163-47a1-a8bf-3cb9edbba877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model,\n",
    "                                                           num_labels=5\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38abae-b140-49a3-aa12-11c4f24e2085",
   "metadata": {},
   "source": [
    "The warning tells us the pre-trained head of the BERT model is discarded, and replaced with a randomly initialized classification head. \n",
    "\n",
    "You will fine-tune this new model head on your sequence classification task, transferring the knowledge of the pretrained model to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50eb25-ed91-4e0f-a081-f399ff05c15a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8765bba-b487-4f7b-99dc-ef24a9b30a97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0252f5-c4c6-446e-a4c9-8859f68d1ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                  evaluation_strategy=\"steps\",\n",
    "                                  num_train_epochs=1,\n",
    "                                  logging_steps=30,\n",
    "                                  use_mps_device=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa056c-1cfc-4273-a4fa-c3105ad0f804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0745e-0a9c-483a-8819-59516859fe0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e6b36-7c75-477a-8edb-664d0180f031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c52cae-deea-436e-a484-50f54ae0d5a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"custom_model\")\n",
    "tokenizer.save_pretrained(\"custom_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c2eaef-9155-455a-80cf-fcd3db298d27",
   "metadata": {},
   "source": [
    "## Now, let's get the model into a pipeline and run it over some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23121b7-79c2-40ba-8a90-b1d6a543818d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = pipeline(\"text-classification\", model=\"custom_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e25ee87-9081-455b-8d2d-b767ce0694a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf(\"The movie is great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bac177-a4da-4f2e-b1a7-c35c3d16fbe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f02739-deba-42b4-bca2-e530a85750d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_datasets[\"test\"][n][\"text\"], tokenized_datasets[\"test\"][n][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c3ab00-1add-4b63-a49b-ed385056fd15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf(tokenized_datasets[\"test\"][n][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d66947-4e7d-407c-a5f0-2f18d60a92a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
